import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
import os

# 1Ô∏è‚É£ ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü ‡¶§‡ßà‡¶∞‡¶ø
data = {
    'sentence': [
        '‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶ú ‡¶ñ‡ßÅ‡¶¨ ‡¶ñ‡ßÅ‡¶∂‡¶ø',
        '‡¶Ü‡¶ú ‡¶ñ‡ßÅ‡¶¨ ‡¶Æ‡¶® ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™',
        '‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶Ö‡¶®‡ßá‡¶ï ‡¶≠‡¶æ‡¶≤‡ßã ‡¶¨‡¶®‡ßç‡¶ß‡ßÅ',
        '‡¶∏‡¶¨ ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶¨‡¶æ‡¶ú‡ßá ‡¶≤‡¶æ‡¶ó‡¶õ‡ßá',
        '‡¶Ü‡¶ú‡¶ï‡ßá‡¶∞ ‡¶¶‡¶ø‡¶®‡¶ü‡¶æ ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞',
        '‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶∞‡ßá‡¶õ‡ßã',
        '‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶Æ‡¶®‡¶ü‡¶æ ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™',
        '‡¶Ü‡¶ú ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£ ‡¶≤‡¶æ‡¶ó‡¶õ‡ßá',
        '‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™',
        '‡¶Ü‡¶ú ‡¶Ü‡¶Æ‡¶ø ‡¶Ö‡¶®‡ßá‡¶ï ‡¶Ü‡¶®‡¶®‡ßç‡¶¶‡¶ø‡¶§'
    ],
    'label': [
        'positive',
        'negative',
        'positive',
        'negative',
        'positive',
        'positive',
        'negative',
        'positive',
        'negative',
        'positive'
    ]
}

df = pd.DataFrame(data)

# 2Ô∏è‚É£ ‡¶ü‡ßã‡¶ï‡ßá‡¶®‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶®
tokenizer = Tokenizer(oov_token="<OOV>")
tokenizer.fit_on_texts(df['sentence'])
sequences = tokenizer.texts_to_sequences(df['sentence'])
padded = pad_sequences(sequences, padding='post')

# 3Ô∏è‚É£ ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶è‡¶®‡¶ï‡ßã‡¶°‡¶ø‡¶Ç
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(df['label'])
labels = to_categorical(labels)

# 4Ô∏è‚É£ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶§‡ßà‡¶∞‡¶ø
vocab_size = len(tokenizer.word_index) + 1
model = Sequential([
    Embedding(vocab_size, 16, input_length=padded.shape[1]),
    GlobalAveragePooling1D(),
    Dense(16, activation='relu'),
    Dense(2, activation='softmax')  # 2 classes: positive, negative
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# 5Ô∏è‚É£ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶á‡¶®‡¶ø‡¶Ç
model.fit(padded, labels, epochs=50, verbose=0)

# 6Ô∏è‚É£ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡¶æ
model.save("bangla_sentiment_model.h5")

# 7Ô∏è‚É£ ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ
def classify(sentence):
    model = load_model("bangla_sentiment_model.h5")
    seq = tokenizer.texts_to_sequences([sentence])
    pad = pad_sequences(seq, maxlen=padded.shape[1], padding='post')
    pred = model.predict(pad)
    label = label_encoder.inverse_transform([np.argmax(pred)])
    return label[0]

# ‚úÖ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶ü‡ßá‡¶∏‡ßç‡¶ü
test_sentence = "‡¶Ü‡¶ú‡¶ï‡ßá ‡¶Æ‡¶® ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™"
print("üîç ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø:", test_sentence)
print("‚û°Ô∏è ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü:", classify(test_sentence))
